



    
| Speaker |  Talk| Abstract   |   
|:----------|:----------:|:----------|
| [**Agnese Barbensi**](https://sites.google.com/view/agnesebarbensi/home) (University of Melbourne, Australia)     |   |  ...   |
|  [**Elizabeth Munch**](http://www.elizabethmunch.com/) (Michigan State University, USA) | The Directional Transform -or- How to look at your data from every direction at once|  The field of topological data analysis (TDA) has emerged as a robust method for measuring the shape of data.  This field of research takes ideas from algebraic topology, in concert with ideas from algorithms, statistics, category theory, and others, to provide methods for quantifying the shape of data. This data can either be cases where shape is obvious, such as studying the shape of a plant seed, or not-so-obvious, such as studying the shape of a reconstructed attractor from a time series. In this talk, we will discuss an idea that has been weaving its way through more and more of the developing TDA methodology; that is, the directional transform. The idea is that since most methods in TDA require input consisting of a topological space with a real valued function, however it is not always obvious what that function should be. In particular, when we are in the setting where we have access to a region of Euclidean space, we can probe the shape with a function and compute a representation such as a persistence diagram, but which direction to choose? The directional transform, first introduced by Turner et al, takes as representation the topological signature for all directions at once. The result is a summary which can be used from theoretical analysis to computational experiments such as being an input to machine learning models. In this talk, we will discuss some uses of the directional transform, including for understanding structures such as X-Ray CT scans of barley seeds as well as for comparing embedded graphs.   | 
|   [**David Gleich**](https://www.cs.purdue.edu/homes/dgleich/) (Purdue University, USA)   | ... | ...          |    
|   [**Jose Perea**](https://www.joperea.com/) (Northeastern University, USA)   | ... |   ...        |    
|  [**Bastian Rieck**](https://bastian.rieck.me/) (AIDOS Lab, Germany)   |  Topology-Based Graph Learning | Topological data analysis is starting to establish itself as a powerful and effective framework in machine learning , supporting the analysis of neural networks, but also driving the development of novel algorithms that incorporate topological characteristics. As a problem class, graph representation learning is of particular interest here, since graphs are inherently amenable to a topological description in terms of their connected components and cycles. This talk will provide an overview of how to address graph learning tasks using machine learning techniques, with a specific focus on how to make such techniques 'topology-aware.' We will discuss how to learn filtrations for graphs and how to incorporate topological information into modern graph neural networks, resulting in provably more expressive algorithms. This talk aims to be accessible to an audience of TDA enthusiasts; prior knowledge of machine learning is helpful but not required.          |    
 |Tamara Kohler  |  Quantum complexity classes and topological data analysis | Recently progress has been made on the long-standing question of the computational complexity of determining homology groups of simplicial complexes, a fundamental task in computational topology, posed by Kaibel and Pfetsch 20 years ago. It was shown that the complexity of versions of the problem is closely linked to quantum complexity classes, suggesting that this seemingly classical problem may in fact be quantum. In this talk I explain why this problem in topology should have anything to do with quantum mechanics, and give an overview of recent results on the complexity of homology. I will discuss what these results suggest for the possibility of quantum advantage in topological data analysis, and open questions. |
  | Colleen M. Farrelly | Hands-On Workshop Series: Forman-Ricci Curvature Applications, BERT, Metric Geometry, and Persistent Homology  | Depending time, we'll 1) overview Forman-Ricci curvature and apply it to social network, spatial, and spatiotemporal data; 2) examine how word embeddings wrangle text data into supervised learning pipelines; 3) explore metric geometry and nearest neighbors algorithms to see how local properties of manifolds impact prediction accuracy; and 4) apply persistent homology to identify transition points in time series data. Most tutorials will be in Python (except #4, which will be in R). Notebooks and data will be provided so students can follow along. |
   | Vanessa Robins | Topological Data Analysis: Introduction and Application  |  Topological Data Analysis has grown out of work focussed on deriving qualitative and yet quantifiable information about the shape of data. The underlying assumption is that knowledge of shape - the way the data are distributed in a space - permits high-level reasoning and modelling of the processes that created this data. The 0-th order aspect of shape is the number pieces: “connected components” to a topologist; “clustering” to a statistician. Higher-order topological aspects of shape are holes, quantified as “non-bounding cycles” in homology theory. These signal the existence of some type of constraint on the data- generating process. Homology lends itself naturally to computer implementation, but its naive application is not robust to perturbations. This inspired the development of persistent homology: an algebraic topological tool that measures changes in the topology of a growing sequence of spaces (a filtration). Persistent homology provides invariants called the barcodes or persistence diagrams that are sets of intervals recording the birth and death parameter values of each homology class in the filtration. It captures information about the shape of data over a range of length scales and enables a distinction between noisy and significant structures that is continuous with respect to perturbation.  This rich geometric summary has found application in fields ranging from astrophysics to materials science and biostatistics.|
 | Sean Andrew Thawe | How To Get Started in Quantum Computing   |  Quantum computers aren’t the next generation of supercomputers but rather they are something else entirely. At the same time quantum computers aren’t developing in isolation of classical computing. Basically, when we look at classical computing at the unit level it stores information in bits. Which is binary, meaning two bits – or two states. Something with two states carries less amount of information. A bit is the smallest unit of classical computation and classical information. Quantum computation and quantum information are built on an analogous concept, the quantum bit, or qubit for short. Just like bits in classical computing, qubits are the core components in quantum computing.  As technology advances into A.I. and ML we find that training models requires quite huge amounts of data. Speeding up the process requires parallelizing the data processing as much as possible. Now with GPU – computer graphics card – its proving to help quite a lot. At the same time, it takes an exponential amount of data to solve the problem and that’s not a good thing, in turn that will mean an exponential number of operations to manipulate them is need. Data parallelization improves performance linearly so it is not a cure for problems with exponential complexity. Quantum computers may have its constraints but like any other technologies it has got Its own advantages. Quantum computers offer us entanglement and interference. Another advantage is superposition. Superposition is the reason that quantum computers can store and manipulate vast amounts of data compare to classical computers.  With superposition, we can encode an exponential amount of information that can scale a solution better than classical computing. There are several ways one can get started in quantum computing and also end up in Quantum Machine Learning. Quantum computing may have a lot to offer but it is still at its infancy stage. Another field developing in quantum computing is Quantum Machine Learning, which is still a theoretical field at the moment. It lies at the intersection quantum computing and machine learning.|
| Carlo Maria Scandolo |  The power of quantum resources | Resource theories are a powerful framework for studying several phenomena in quantum information. In this talk, I present some of my latest research results in this area, showing how resource theories can be fruitfully employed to open new research directions, both in quantum information and beyond. |
 | Woojin Kim | Persistence diagrams via limit-to-colimit maps and Möbius inversions  | The persistence diagram (equivalently barcode) has been one of the most prevalent objects in topological data analysis (TDA) as an object which summarizes features of a persistence module. With the goal of adapting the idea behind persistent homology to the study of wider types of data (e.g. time-varying point clouds), variants of the indexing set of persistence modules inevitably occur, leading for example to multiparameter persistence and zigzag persistence. However, it is not always evident how to define a notion of persistence diagram for such variants.
This talk will introduce a generalized notion of persistence diagram for many of such variants which arises through exploiting the principle of inclusion-exclusion from combinatorics and the notion of (co)limit from category theory. We describe how this resulting generalized persistence diagram subsumes some other well-known invariants of 2-parameter persistence modules and how it mediates between 2-parameter persistence and zigzag persistence. This talk is based on joint work with Nate Clause, Tamal Dey, Facundo Mémoli, and Samantha Moore. |
 | ... | ...  | ... |
 | ... | ...  | ... |
 | ... | ...  | ... |
      
